{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install kagglehub\n",
    "# pip install numpy == 1.26.4\n",
    "# pip install pandas\n",
    "# pip install plotly >= 24.2\n",
    "# pip install statsmodel == 2.2.3\n",
    "# pip install scikit-surprise == 1.1.4\n",
    "# pip install scikit-learn == 1.6.1\n",
    "# pip install tqdm == 4.67.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"saurav9786/amazon-product-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "# import dash\n",
    "# import dash_html_components as html\n",
    "# import dash_cytoscape as cyto\n",
    "# from matplotlib import colors as mcolors\n",
    "from itertools import zip_longest\n",
    "# from ast import literal_eval\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = pd.read_csv('data/ratings_Electronics.csv', names=['userId', 'productId','Rating','timestamp'])\n",
    "rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.describe()['Rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rating of the product range from 0 to 1, and majority of the products are rated as 5.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values\n",
    "print('Number of missing values across columns: \\n', rating_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values that needs to be handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the rating distribution\n",
    "\n",
    "data = rating_df.groupby(['Rating']).size().reset_index(name='number of ratings')\n",
    "\n",
    "fig = px.bar(data, x=\"Rating\", y=\"number of ratings\", color=\"Rating\", color_continuous_scale= px.colors.sequential.Blues)\n",
    "fig.update_layout(title_text='Rating Distribution',\n",
    "                xaxis_title='Rating',\n",
    "                yaxis_title='Number of Ratings',\n",
    "                showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users tend to give positive reviews unless the product is highly unsatisfactory, in which case they rate it as 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distinct count\n",
    "print(\"Total data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal number of ratings :\",rating_df.shape[0])\n",
    "print(\"Total number of users :\", len(np.unique(rating_df.userId)))\n",
    "print(\"Total number of products :\", len(np.unique(rating_df.productId)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top users who provided the most ratings and average ratings\n",
    "user_rating_stats = rating_df.groupby(by='userId').agg(total_ratings=('Rating', 'count'),\n",
    "                                                       avg_rating=('Rating', 'mean'))\\\n",
    "                                                    .sort_values(by='total_ratings', ascending=False)\\\n",
    "                                                    .reset_index()\n",
    "\n",
    "user_rating_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating_stats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the median and Q3, users normally rated once or twice, which may indicate high sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(user_rating_stats, \n",
    "                 x=\"avg_rating\",\n",
    "                 y=\"total_ratings\",\n",
    "                 title=\"User Rating Behavior\",\n",
    "                 labels={\"total_ratings\": \"Total Ratings Per User\", \"avg_rating\": \"Average Rating\"},\n",
    "                 opacity=0.6,\n",
    "                 hover_data=[\"userId\"])\n",
    "\n",
    "fig.update_layout(xaxis=dict(title=\"Average Rating\", type=\"log\"),\n",
    "                  yaxis=dict(title=\"Total Ratings Per User\"),\n",
    "                  template=\"plotly_white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top products who provided the most ratings and average ratings\n",
    "product_rating_stats = rating_df.groupby(by='productId').agg(total_ratings=('Rating', 'count'),\n",
    "                                                       avg_rating=('Rating', 'mean'))\\\n",
    "                                                    .sort_values(by='total_ratings', ascending=False)\\\n",
    "                                                    .reset_index()\n",
    "\n",
    "product_rating_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_rating_stats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most products received around two ratings each, as reflected in the median, though the mean is much higher. This indicates a long-tail distribution, where a few products are rated very frequently, while most receive only a few ratings. Indicating high sparsity as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(product_rating_stats, x=\"total_ratings\", nbins=100, log_y=True, \n",
    "                   title=\"Distribution of Total Ratings per Product\",\n",
    "                   labels={\"total_ratings\": \"Total Ratings per Product\"},\n",
    "                   opacity=0.7)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(product_rating_stats, \n",
    "                 x=\"avg_rating\",\n",
    "                 y=\"total_ratings\",\n",
    "                 title=\"Products Overall Rating\",\n",
    "                 labels={\"total_ratings\": \"Total Ratings Per Product\", \"avg_rating\": \"Average Rating\"},\n",
    "                 opacity=0.6,\n",
    "                 hover_data=[\"productId\"])\n",
    "\n",
    "fig.update_layout(xaxis=dict(title=\"Average Rating\", type=\"log\"),\n",
    "                  yaxis=dict(title=\"Total Ratings Per Product\"),\n",
    "                  template=\"plotly_white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data sparsity\n",
    "sparsity = rating_df.shape[0] / (rating_df['userId'].nunique() * rating_df['productId'].nunique())\n",
    "print(f\"Dataset sparsity: {sparsity:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparsity value 0.000004 (or 0.0004%) means that only a tiny fraction of possible user-product interactions have ratings. This indicates a highly sparse dataset.\n",
    "\n",
    "In general, sparse data means that the data contains missing values, in this case, there's a lot of missing ratings because most users rated only a few products and most products receive ratings from only a few users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "One way to handle sparse data is to filter out some users/products, these users maybe inactive/rarely active while the products may not be popular to determine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the total ratings per user are negatively skewed, we will retain only active users who have more than 3 reviews.\n",
    "user_product_counts = rating_df.groupby('userId')['productId'].nunique().reset_index()\n",
    "user_product_counts.rename(columns={'productId': 'total_products'}, inplace=True)\n",
    "filtered_users = user_product_counts[user_product_counts['total_products'] >= 3]['userId']\n",
    "rating_df_filtered = rating_df[rating_df['userId'].isin(filtered_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# products with very few reviews may make it difficult to determine whether they genuinely attract user interest. \n",
    "# given that 50% of products have at least 2 reviews, I will use this as the threshold.\n",
    "product_review_counts = rating_df.groupby('productId').value_counts().rename('review_count')\n",
    "product_review_counts = rating_df.groupby('productId').size().reset_index(name='review_count').query('review_count > 2')\n",
    "rating_df_filtered = rating_df_filtered[rating_df_filtered['productId'].isin(product_review_counts['productId'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users with an unusually high number of reviews may be outliers or non-genuine users, so they will be filtered out.\n",
    "Q1 = user_rating_stats[\"total_ratings\"].quantile(0.25)\n",
    "Q3 = user_rating_stats[\"total_ratings\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_threshold = Q3 + 1.5 * IQR\n",
    "\n",
    "filtered_users = user_rating_stats[user_rating_stats[\"total_ratings\"] <= upper_threshold][\"userId\"]\n",
    "rating_df_filtered = rating_df_filtered[rating_df_filtered[\"userId\"].isin(filtered_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distinct count after cleaning\n",
    "print(\"Total data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal number of ratings :\",rating_df_filtered.shape[0])\n",
    "print(\"Total number of users :\", len(np.unique(rating_df_filtered.userId)))\n",
    "print(\"Total number of products :\", len(np.unique(rating_df_filtered.productId)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering (Item-Based)\n",
    "Collaborative Filtering aims to predict missing ratings in a user-item interaction matrix by leveraging similarities between users or items.\n",
    "I will start with a memory-based approach, specifically Item-Based Collaborative Filtering (Item-Based CF).\n",
    "\n",
    "Difference Between User-Based and Item-Based CF\n",
    "- User-Based CF: Finds users with similar preferences and recommends items liked by those users.\n",
    "- Item-Based CF: Finds items that are similar to what a user has already rated highly and recommends those items.\n",
    "\n",
    "In this case, I will predict the ratings for products that users have not yet rated by looking at the similarity between items. If a user has given high ratings to certain products, Item-Based CF will recommend other similar products based on those preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, KNNWithMeans\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Process for Model Building & Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take a proportion of data, since the data size is too big (size were in TiB)\n",
    "sample_df = rating_df_filtered.sample(frac=0.10, random_state=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(sample_df[\"Rating\"].min(), rating_df[\"Rating\"].max()))\n",
    "data = Dataset.load_from_df(sample_df[['userId', 'productId', 'Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.3, random_state=2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {\"name\": \"cosine\",\n",
    "               \"user_based\": False}\n",
    "\n",
    "model = KNNWithMeans(sim_options=sim_options)\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the rating scale from 1-5, a RMSE of 1.33 means, on average, the predicted ratings deviate by ~1.33 points from actual ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find similar products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = model.sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_products(product_id, top_n=5):\n",
    "    try:\n",
    "        inner_id = model.trainset.to_inner_iid(product_id)\n",
    "        similarity_scores = sim_matrix[inner_id]\n",
    "        similar_items = sorted(list(enumerate(similarity_scores)), key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
    "        return [model.trainset.to_raw_iid(inner_idx) for inner_idx, _ in similar_items]\n",
    "    except:\n",
    "        return \"Product not found in training data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of the first few products' similar products\n",
    "for product in sample_df[\"productId\"][:10]:\n",
    "    print(f\"Top 5 similar products to {product}: {get_similar_products(product)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering (with TimeSVD++)\n",
    "Collaborative Filtering aims to predict missing ratings in a user-item interaction matrix by leveraging similarities between users or items.\n",
    "Given the rating scale of 1 to 5, the previous model could be improved, as a 1.33 standard deviation may misinterpret a mid-range product as good or a bad product as acceptable. \n",
    "I will explore a model-based approach that considers timestamps. Since scikit-surprise only has SVD++, hence additional preprocessing is needed to use to incorporate time variable, which will become TimeSVD++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import coo_matrix\n",
    "from surprise import SVDpp, Dataset, Reader\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the whole dataset since TimeSVD++can handle larger data\n",
    "df = rating_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df['rating_scaled'] = scaler.fit_transform(df[['Rating']]) \n",
    "df[['Rating', 'rating_scaled']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TimeSVD++ modifies the bias term to include time-dependent user preferences\n",
    "- Time-based normalization helps capture how a user's preferences shift over time, this means if a user’s rating pattern changes over time, their bias will be adjusted dynamically.\n",
    "- Mean timestamp per user acts as a reference point to measure how much a given rating's timestamp deviates from the user’s usual behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute time based normalization\n",
    "# mean timestamp\n",
    "user_mean_time = df.groupby('userId')['timestamp'].mean()\n",
    "df['time_dev'] = df.apply(lambda row: row['timestamp'] - user_mean_time[row['userId']], axis=1)\n",
    "\n",
    "# scale time deviation\n",
    "scaler = MinMaxScaler()\n",
    "df['time_dev_norm'] = scaler.fit_transform(df[['time_dev']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust rating based on time bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_u = 0.004 # alpha_u is a time decay parameter that adjusts the influence of time on the user's rating behaviour\n",
    "df['adjusted_rating'] = df['Rating'] + (alpha_u * df['time_dev_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[['userId', 'productId', 'adjusted_rating']], reader)\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVDpp(n_factors=100, lr_all=0.005, reg_all=0.02, verbose=True)\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.test(testset)\n",
    "rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVDpp(n_factors=40, lr_all=0.01, reg_all=0.2, n_epochs=25)\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.test(testset)\n",
    "rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE has reduce from 1.33 to 1.25. On average, the predicted ratings deviate by ~1.25 points from actual ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find similar products\n",
    "The SVDpp model learns latent factor representations for both users and products, hence I can access product embeddings (also called item factors) from the trained model to find similar products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors = model.pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because data is too large, unable to pivot directly, using coo_matrix to pivot\n",
    "user_ids = df['userId'].astype(\"category\").cat.codes # to use coo_matrix, will need to convert to integer\n",
    "product_ids = df['productId'].astype(\"category\").cat.codes # to use coo_matrix, will need to convert to integer\n",
    "ratings = df['Rating'].values\n",
    "\n",
    "# use this to pivot\n",
    "sparse_matrix = coo_matrix((ratings, (user_ids, product_ids))) # this will save in memory\n",
    "\n",
    "# store the original ids to map back after pivot\n",
    "user_mapping = dict(enumerate(df['userId'].astype(\"category\").cat.categories))\n",
    "product_mapping = dict(enumerate(df['productId'].astype(\"category\").cat.categories))\n",
    "\n",
    "print(f\"Sparse Matrix Shape: {sparse_matrix.shape}\")\n",
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cosine similarity can't handle well with large dataset, I am using Nearest Neighors to find top 5 similar products. This will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# use Nearest Neighbors model to fin\n",
    "k = 5 # k = 3,5,10\n",
    "knn = NearestNeighbors(n_neighbors=k+1, metric='cosine', algorithm='brute')\n",
    "knn.fit(sparse_matrix.T)\n",
    "\n",
    "# get top k similar products\n",
    "distances, indices = knn.kneighbors(sparse_matrix.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each product, store the top 5 similar products and check whether users who buy this product also purchase any of its similar products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_products_dict = {}\n",
    "for i, product_idx in enumerate(indices): # mapping back the original product IDs\n",
    "    original_product_id = product_mapping[i] \n",
    "    similar_product_ids = [product_mapping[idx] for idx in product_idx[1:]] # exclude itself\n",
    "    similar_products_dict[original_product_id] = similar_product_ids\n",
    "\n",
    "similar_products_df = pd.DataFrame.from_dict(similar_products_dict, orient='index')\n",
    "similar_products_df.columns = [f\"Similar_Product_{i+1}\" for i in range(k)]\n",
    "similar_products_df.reset_index(inplace=True)\n",
    "similar_products_df.rename(columns={\"index\": \"productId\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_products_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given each product's top 5 similar products, calculate the overlap purchase (users who bought at least one recommended product along with the original) vs. the non-overlap purchase (users who only bought the original product but not the recommendations).  \n",
    "\n",
    "From there, determine the Overlap Rate, Precision of the top 5 products, Hit Rate, and Lift Score of the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_purchases_dict = df.groupby(\"userId\")[\"productId\"].apply(list).to_dict()\n",
    "\n",
    "def compute_overlap_metrics(user_purchases_dict, similar_products_dict):\n",
    "    total_users = len(user_purchases_dict)  # Total users\n",
    "    overlapping_users = 0\n",
    "    total_recommended_purchases = 0\n",
    "    total_recommendations_made = 0\n",
    "    total_actual_purchases = 0\n",
    "    users_with_recommendations = 0\n",
    "\n",
    "    for user, products in user_purchases_dict.items():\n",
    "        purchased_set = set(products)\n",
    "        total_actual_purchases += len(purchased_set)\n",
    "        overlap_found = False\n",
    "        received_recommendations = False\n",
    "\n",
    "        for product in products:\n",
    "            if product in similar_products_dict:\n",
    "                recommended_products = set(similar_products_dict[product])\n",
    "                received_recommendations = True\n",
    "                total_recommendations_made += len(recommended_products)\n",
    "\n",
    "                # find which recommended products the user actually bought\n",
    "                purchased_recommendations = purchased_set.intersection(recommended_products)\n",
    "                total_recommended_purchases += len(purchased_recommendations)\n",
    "\n",
    "                if purchased_recommendations:\n",
    "                    overlap_found = True\n",
    "\n",
    "        if received_recommendations:\n",
    "            users_with_recommendations += 1\n",
    "        if overlap_found:\n",
    "            overlapping_users += 1\n",
    "\n",
    "    overlap_rate = overlapping_users / total_users if total_users else 0\n",
    "    precision_at_k = total_recommended_purchases / total_recommendations_made if total_recommendations_made else 0\n",
    "    recall_at_k = total_recommended_purchases / total_actual_purchases if total_actual_purchases else 0\n",
    "    hit_rate = overlapping_users / users_with_recommendations if users_with_recommendations else 0\n",
    "\n",
    "    return overlap_rate, precision_at_k, recall_at_k, hit_rate\n",
    "\n",
    "# compute model performance\n",
    "observed_overlap_rate, precision_at_k, recall_at_k, hit_rate = compute_overlap_metrics(user_purchases_dict, similar_products_dict)\n",
    "\n",
    "# compute popularity-based random recommendations\n",
    "popular_products = df[\"productId\"].value_counts().index[:50]  # Top 50 popular products\n",
    "random_recommendations = {\n",
    "    product: np.random.choice(popular_products, 5, replace=False).tolist()\n",
    "    for product in similar_products_dict.keys()\n",
    "}\n",
    "random_overlap_rate, _, _, _ = compute_overlap_metrics(user_purchases_dict, random_recommendations)\n",
    "\n",
    "# compute lift score\n",
    "lift_score = observed_overlap_rate / random_overlap_rate if random_overlap_rate else 0\n",
    "\n",
    "print(f\"Overlap Rate: {observed_overlap_rate:.4f}\")\n",
    "print(f\"Hit Rate: {hit_rate:.4f}\")\n",
    "# overlap rate and hit rate should be the same, since every user will receive recommendation based on what they purchase\n",
    "print(f\"Precision @ 5: {precision_at_k:.4f}\")\n",
    "print(f\"Recall @ 5: {recall_at_k:.4f}\")\n",
    "print(f\"Lift Score: {lift_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overlap Rate / Hit Rate: 60.17% of users had at least one correct recommendation in their top 5. This means most users received at least one good suggestion.\n",
    "- Precision: 12.42% of recommended products were actually purchased by users.\n",
    "- Recall: 62.12% of all purchased products were successfully recommended. This means that across all purchases, a large portion was correctly predicted.\n",
    "- Lift Score: The model is 13x better than randomly recommending popular products. A lift score this high indicates strong recommendation effectiveness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
